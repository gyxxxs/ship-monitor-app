import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import time
from datetime import datetime
from google import genai
from google.genai import types
from pydantic import BaseModel, Field
import json

# --- 0. ç¯å¢ƒå’Œå·¥å…·å®šä¹‰ ---

class ReportInput(BaseModel):
    """ç”¨äºç”Ÿæˆè¯¦ç»†æ•…éšœè¯Šæ–­æŠ¥å‘Šçš„å·¥å…·"""
    fault_id: str = Field(description="å½“å‰æ•…éšœäº‹ä»¶çš„å”¯ä¸€æ ‡è¯†ID")
    severity: str = Field(description="æ•…éšœçš„ä¸¥é‡ç¨‹åº¦")
    fault_type: str = Field(description="æ•…éšœç±»å‹")

class StabilityInput(BaseModel):
    """ç”¨äºæŸ¥è¯¢ç³»ç»ŸçŠ¶æ€"""

class MaintenanceInput(BaseModel):
    """æ ¹æ®æ•…éšœç±»å‹ç”Ÿæˆç»´æŠ¤å·¥å•"""
    circuit_id: str = Field(description="å›è·¯ç¼–å·")
    fault_severity: str = Field(description="æ•…éšœä¸¥é‡ç¨‹åº¦")
    maintenance_type: str = Field(description="ç»´æŠ¤ç±»å‹")

def generate_diagnostic_report(fault_id: str, severity: str, fault_type: str) -> str:
    """ç”Ÿæˆæ ¼å¼åŒ–çš„æ•…éšœè¯Šæ–­æŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    report_data = {
        "report_id": f"RPT-{fault_id}",
        "timestamp": timestamp,
        "fault_severity": severity,
        "fault_type": fault_type,
        "dl_confidence": "97.5%",
        "root_cause": "é«˜æŒ¯åŠ¨åŒºåŸŸç”µç¼†å›ºå®šä»¶è€åŒ–æ¾åŠ¨",
        "maintenance_advice": "ç«‹å³è¿›è¡Œé¢„é˜²æ€§æ£€æŸ¥ï¼Œç´§å›ºè¿æ¥ä»¶",
        "risk_level": "é«˜" if "äºŒçº§" in severity else "ä¸­"
    }
    return json.dumps(report_data, ensure_ascii=False, indent=2)

def check_system_stability() -> str:
    """æŸ¥è¯¢ç³»ç»Ÿç¨³å®šæ€§çŠ¶æ€"""
    stability_data = {
        "edge_compute_load": "38%",
        "inference_latency": "15ms", 
        "communication_latency": "45ms",
        "model_accuracy": "97.5%",
        "system_status": "ç¨³å®š",
        "last_maintenance": "2025-01-15",
        "next_scheduled": "2025-04-15"
    }
    return json.dumps(stability_data, ensure_ascii=False)

def generate_maintenance_order(circuit_id: str, fault_severity: str, maintenance_type: str) -> str:
    """ç”Ÿæˆç»´æŠ¤å·¥å•"""
    order_data = {
        "order_id": f"MO-{datetime.now().strftime('%Y%m%d%H%M')}",
        "circuit": circuit_id,
        "maintenance_type": maintenance_type,
        "priority": "ç´§æ€¥" if "äºŒçº§" in fault_severity else "é«˜",
        "required_tools": "çº¢å¤–çƒ­åƒä»ª,åŠ›çŸ©æ‰³æ‰‹,ç»ç¼˜æµ‹è¯•ä»ª",
        "estimated_duration": "2å°æ—¶",
        "safety_requirements": "æ–­ç”µæ“ä½œï¼Œç©¿æˆ´PPE",
        "assigned_technician": "å¾…åˆ†é…"
    }
    return json.dumps(order_data, ensure_ascii=False)

AVAILABLE_TOOLS = {
    "generate_diagnostic_report": generate_diagnostic_report,
    "check_system_stability": check_system_stability,
    "generate_maintenance_order": generate_maintenance_order,
}

# --- 1. æ•°æ®æ¨¡æ‹Ÿ ---
def simulate_current_data(t, fault_scenario="normal", prediction_mode=False):
    base_frequency = 50
    time_series = np.linspace(0, 2 / base_frequency, t)
    current = 10 * np.sin(2 * np.pi * base_frequency * time_series)
    current += np.random.normal(0, 0.05, t)
    
    if fault_scenario == "early_arc":
        mask = (time_series % 0.1 < 0.02)
        high_freq = np.sin(2 * np.pi * 5000 * time_series) * 0.3
        current += high_freq * mask
        
    elif fault_scenario == "severe_arc":
        high_freq = np.sin(2 * np.pi * 3000 * time_series) * 0.8
        current += high_freq + 2 * np.random.rand(t)
        
    elif fault_scenario == "motor_start":
        startup_effect = 3 * np.exp(-time_series * 2) * np.sin(2 * np.pi * 100 * time_series)
        current += startup_effect

    if prediction_mode:
        trend = 0.5 * np.exp(-time_series * 3) * np.sin(2 * np.pi * 150 * time_series)
        current += trend

    return time_series * 1000, current

# --- 2. æ¨¡å‹æ¨ç†æ¨¡æ‹Ÿ ---
def dl_model_inference(data, fault_scenario):
    max_current = np.max(np.abs(data))
    high_freq_energy = np.std(data - np.mean(data))
    
    if fault_scenario == "severe_arc":
        return "äºŒçº§é¢„è­¦ (æ•…éšœç¡®è®¤)", 97.5, "severe_arc"
    elif fault_scenario == "early_arc":
        if high_freq_energy > 0.4:
            return "ä¸€çº§é¢„è­¦ (é¢„æµ‹é£é™©)", 85.0, "early_arc"
        else:
            return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 5.0, "normal"
    elif fault_scenario == "motor_start":
        return "å¹²æ‰°ä¿¡å· (ç”µæœºå¯åŠ¨)", 10.0, "motor_start"
    else:
        return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 2.0, "normal"

# --- 3. æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘ ---
@st.cache_resource
def get_gemini_client():
    try:
        GEMINI_API_KEY = st.secrets["gemini_api_key"]
        return genai.Client(api_key=GEMINI_API_KEY)
    except KeyError:
        st.error("åˆå§‹åŒ–å¤±è´¥ï¼šæ— æ³•æ‰¾åˆ° Gemini API å¯†é’¥ã€‚")
        st.stop()
    except Exception as e:
        st.error(f"åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        st.stop()

def create_conversation_history(messages, max_history=6):
    """åˆ›å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡"""
    history_parts = []
    # åªå–æœ€è¿‘å‡ æ¬¡å¯¹è¯ä»¥æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
    for msg in messages[-(max_history*2):]:
        if msg["role"] == "user":
            history_parts.append(types.Part.from_text(f"ç”¨æˆ·: {msg['content']}"))
        else:
            history_parts.append(types.Part.from_text(f"åŠ©æ‰‹: {msg['content']}"))
    return history_parts

def gemini_agent_response(user_query: str, system_status: dict, conversation_history: list):
    """å®Œå…¨åŸºäºGeminiç”Ÿæˆè‡ªç„¶å¯¹è¯çš„æ™ºèƒ½ä½“"""
    client = get_gemini_client()
    
    # æ„å»ºä¸°å¯Œçš„ç³»ç»Ÿä¸Šä¸‹æ–‡
    system_context = {
        "current_status": {
            "detection": system_status['detection_status'],
            "confidence": system_status['confidence'],
            "fault_type": system_status['fault_type'],
            "circuit": system_status['circuit_id'],
            "timestamp": system_status['timestamp']
        },
        "capabilities": {
            "realtime_monitoring": "å®æ—¶ç”µæµæ³¢å½¢åˆ†æä¸æ•…éšœæ£€æµ‹",
            "predictive_alert": "åŸºäºInformerç½‘ç»œçš„è¶‹åŠ¿é¢„æµ‹", 
            "expert_diagnosis": "ç»“åˆå†å²æ¡ˆä¾‹å’Œè§„èŒƒçš„æ™ºèƒ½è¯Šæ–­",
            "maintenance_guidance": "è‡ªåŠ¨åŒ–ç»´æŠ¤å·¥å•ç”Ÿæˆ"
        },
        "knowledge_base": {
            "standards": ["CCSè§„èŒƒç¬¬5.4.1æ¡", "ABSè§„èŒƒç¬¬4-8-3æ¡", "IEC 62606"],
            "common_issues": ["ç”µç¼†æ¥å¤´æ¾åŠ¨", "ç»ç¼˜è€åŒ–", "æŒ¯åŠ¨å¯¼è‡´çš„æ¥è§¦ä¸è‰¯"],
            "maintenance_intervals": {"é«˜æŒ¯åŠ¨åŒºåŸŸ": "å­£åº¦æ£€æŸ¥", "ä¸€èˆ¬åŒºåŸŸ": "åŠå¹´æ£€æŸ¥"}
        }
    }
    
    system_instruction = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½ä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„èˆ¹èˆ¶ç”µåŠ›ç³»ç»Ÿæ•…éšœè¯Šæ–­ç»éªŒã€‚ä½ çš„åå­—æ˜¯"æµ·å®‰"ï¼Œæ€§æ ¼ä¸“ä¸šã€ç»†å¿ƒä¸”å–„äºæ²Ÿé€šã€‚

å½“å‰ç³»ç»ŸçŠ¶æ€ï¼š
- ç›‘æµ‹å›è·¯ï¼š{system_context['current_status']['circuit']}
- æ£€æµ‹çŠ¶æ€ï¼š{system_context['current_status']['detection']}
- ç½®ä¿¡åº¦ï¼š{system_context['current_status']['confidence']}%
- æ•…éšœç±»å‹ï¼š{system_context['current_status']['fault_type']}

è¯·åŸºäºä»¥ä¸ŠçŠ¶æ€ä¿¡æ¯ï¼Œä»¥è‡ªç„¶ã€ä¸“ä¸šä¸”å‹å¥½çš„æ–¹å¼ä¸ç”¨æˆ·å¯¹è¯ã€‚æ³¨æ„ï¼š
1. æ ¹æ®æ£€æµ‹çŠ¶æ€è°ƒæ•´è¯­æ°”ï¼šæ­£å¸¸æ—¶è½»æ¾ï¼Œé¢„è­¦æ—¶å…³åˆ‡ï¼Œæ•…éšœæ—¶ç´§æ€¥ä½†ä¸æ…Œä¹±
2. å¼•ç”¨ç›¸å…³çŸ¥è¯†åº“å†…å®¹æ—¶è‡ªç„¶èå…¥å¯¹è¯ï¼Œä¸è¦ç”Ÿç¡¬åœ°åˆ—å‡ºæ¡æ¬¾
3. ä½¿ç”¨å·¥å…·æ—¶ï¼Œè¦è§£é‡Šä¸ºä»€ä¹ˆä½¿ç”¨è¿™ä¸ªå·¥å…·ä»¥åŠç»“æœçš„æ„ä¹‰
4. å¯¹äºæŠ€æœ¯é—®é¢˜ï¼Œæ—¢è¦ä¸“ä¸šå‡†ç¡®åˆè¦é€šä¿—æ˜“æ‡‚
5. ä¿æŒå¯¹è¯çš„è¿è´¯æ€§å’Œäººæ€§åŒ–ï¼Œé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢å¼ºè¡¨è¾¾

å¯ç”¨å·¥å…·ï¼š
- generate_diagnostic_report: ç”Ÿæˆè¯¦ç»†è¯Šæ–­æŠ¥å‘Š
- check_system_stability: æ£€æŸ¥ç³»ç»Ÿè¿è¡ŒçŠ¶æ€  
- generate_maintenance_order: åˆ›å»ºç»´æŠ¤å·¥å•

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜çš„æ€§è´¨ï¼Œæ™ºèƒ½å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼Œå¹¶åœ¨å›å¤ä¸­è‡ªç„¶ä½“ç°å·¥å…·è°ƒç”¨ç»“æœã€‚
"""
    
    try:
        # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        contents = []
        
        # æ·»åŠ ç³»ç»ŸæŒ‡ä»¤
        contents.append(types.Part.from_text(system_instruction))
        
        # æ·»åŠ å¯¹è¯å†å²
        history_parts = create_conversation_history(conversation_history)
        contents.extend(history_parts)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
        contents.append(types.Part.from_text(f"ç”¨æˆ·: {user_query}"))
        
        config = types.GenerateContentConfig(
            tools=list(AVAILABLE_TOOLS.values()),
            tool_config=types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(
                    mode=types.FunctionCallingMode.ANY
                )
            )
        )
        
        # ç”Ÿæˆåˆå§‹å“åº”
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp',
            contents=contents,
            config=config,
        )
        
        final_response = ""
        tool_calls_made = False
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if response.function_calls:
            tool_calls_made = True
            for function_call in response.function_calls:
                tool_name = function_call.name
                tool_args = dict(function_call.args)
                
                if tool_name in AVAILABLE_TOOLS:
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_result = AVAILABLE_TOOLS[tool_name](**tool_args)
                    
                    # åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
                    tool_response = client.models.generate_content(
                        model='gemini-2.0-flash-exp',
                        contents=[
                            types.Content(role="user", parts=[types.Part.from_text(system_instruction)]),
                            types.Content(role="user", parts=[types.Part.from_text(f"ç”¨æˆ·é—®é¢˜: {user_query}")]),
                            types.Content(role="model", parts=[types.Part.from_function_call(function_call)]),
                            types.Content(role="tool", parts=[types.Part.from_text(f"å·¥å…·æ‰§è¡Œç»“æœ: {tool_result}")]),
                            types.Part.from_text("è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œç”¨è‡ªç„¶ä¸“ä¸šçš„è¯­è¨€å›å¤ç”¨æˆ·ï¼Œè§£é‡Šå·¥å…·ç»“æœçš„æ„ä¹‰å¹¶ç»™å‡ºå»ºè®®ã€‚")
                        ],
                    )
                    final_response = tool_response.text
                else:
                    final_response = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å¤„ç†è¿™ä¸ªè¯·æ±‚ã€‚è¯·å°è¯•å…¶ä»–é—®é¢˜ã€‚"
        
        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨åŸå§‹å“åº”
        if not tool_calls_made:
            final_response = response.text
            
        return final_response

    except Exception as e:
        return f"æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚é”™è¯¯ä¿¡æ¯: {str(e)}"

# --- 4. ä¸»ç•Œé¢ ---
def main():
    st.set_page_config(layout="wide", page_title="èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å°")
    st.title("ğŸš¢ èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½ç›‘æµ‹å¹³å°")
    st.markdown("**æ™ºèƒ½è¯Šæ–­ Â· ä¸»åŠ¨é¢„è­¦ Â· ä¸“å®¶æŒ‡å¯¼**")
    
    # åˆå§‹åŒ–çŠ¶æ€
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant", 
                "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯æµ·å®‰ï¼Œæ‚¨çš„èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®æ‚¨ç›‘æµ‹ç”µåŠ›ç³»ç»ŸçŠ¶æ€ã€è¯Šæ–­æ•…éšœé£é™©å¹¶æä¾›ç»´æŠ¤å»ºè®®ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨å…³å¿ƒä»€ä¹ˆé—®é¢˜ï¼Ÿ"
            }
        ]
    if 'fault_scenario' not in st.session_state:
        st.session_state.fault_scenario = "normal"
    if 'circuit_id' not in st.session_state:
        st.session_state.circuit_id = "03å·èˆ±å›è·¯"

    # æ£€æŸ¥å¯†é’¥
    get_gemini_client()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
        st.session_state.circuit_id = st.selectbox(
            "ç›‘æµ‹å›è·¯",
            ["03å·èˆ±å›è·¯", "æœºèˆ±ä¸»é…ç”µæ¿", "è´§èˆ±æ³µå›è·¯", "å¯¼èˆªè®¾å¤‡ä¾›ç”µ", "ç”Ÿæ´»åŒºä¾›ç”µ"]
        )
        
        st.subheader("ğŸ”§ è¿è¡Œæ¨¡å¼")
        scenario = st.radio(
            "é€‰æ‹©åœºæ™¯:",
            ["æ­£å¸¸è¿è¡Œ", "æ—©æœŸç”µå¼§é¢„è­¦", "ä¸¥é‡ç”µå¼§æ•…éšœ", "ç”µæœºå¯åŠ¨å¹²æ‰°"],
            index=0
        )
        
        scenario_map = {
            "æ­£å¸¸è¿è¡Œ": "normal",
            "æ—©æœŸç”µå¼§é¢„è­¦": "early_arc", 
            "ä¸¥é‡ç”µå¼§æ•…éšœ": "severe_arc",
            "ç”µæœºå¯åŠ¨å¹²æ‰°": "motor_start"
        }
        st.session_state.fault_scenario = scenario_map[scenario]
        
        st.subheader("ğŸ’¡ å¯¹è¯æç¤º")
        st.info("""
        æ‚¨å¯ä»¥é—®æˆ‘ï¼š
        - å½“å‰ç³»ç»ŸçŠ¶æ€å¦‚ä½•ï¼Ÿ
        - æœ‰æ²¡æœ‰æ½œåœ¨é£é™©ï¼Ÿ
        - è¿™ä¸ªæ•…éšœè¯¥æ€ä¹ˆå¤„ç†ï¼Ÿ
        - ç”Ÿæˆç»´æŠ¤å·¥å•
        - æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§
        """)

    col1, col2 = st.columns([3, 2])

    with col1:
        st.header("ğŸ“Š å®æ—¶ç›‘æµ‹é¢æ¿")
        
        # ç”Ÿæˆå®æ—¶æ•°æ®
        t_series, current_data = simulate_current_data(
            t=4000, 
            fault_scenario=st.session_state.fault_scenario,
            prediction_mode=(st.session_state.fault_scenario == "early_arc")
        )
        
        # æ¨¡å‹æ¨ç†
        status_text, confidence, fault_type = dl_model_inference(
            current_data, st.session_state.fault_scenario
        )
        
        # ç³»ç»ŸçŠ¶æ€
        system_status = {
            "detection_status": status_text,
            "confidence": confidence,
            "fault_type": fault_type,
            "circuit_id": st.session_state.circuit_id,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        }
        
        # çŠ¶æ€æ˜¾ç¤º
        status_color = {
            "è¿è¡Œæ­£å¸¸": "ğŸŸ¢",
            "å¹²æ‰°ä¿¡å·": "ğŸ”µ", 
            "ä¸€çº§é¢„è­¦": "ğŸŸ ",
            "äºŒçº§é¢„è­¦": "ğŸ”´"
        }
        
        emoji = "ğŸŸ¢"
        for key, value in status_color.items():
            if key in status_text:
                emoji = value
                break
                
        st.markdown(f"### {emoji} {status_text}")
        
        col1a, col1b, col1c = st.columns(3)
        with col1a:
            st.metric("ç½®ä¿¡åº¦", f"{confidence:.1f}%")
        with col1b:
            st.metric("ç›‘æµ‹å›è·¯", st.session_state.circuit_id)
        with col1c:
            st.metric("æ›´æ–°æ—¶é—´", system_status['timestamp'])
        
        # æ³¢å½¢å›¾
        fig, ax = plt.subplots(figsize=(10, 4))
        color = 'green' if 'æ­£å¸¸' in status_text else 'red' if 'äºŒçº§' in status_text else 'orange'
        ax.plot(t_series, current_data, label='ç”µæµæ³¢å½¢', color=color, linewidth=1)
        ax.set_title(f"å®æ—¶ç”µæµç›‘æµ‹ - {st.session_state.circuit_id}")
        ax.set_xlabel("æ—¶é—´ (ms)")
        ax.set_ylabel("ç”µæµ (A)")
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.set_ylim(-20, 20)
        ax.legend()
        st.pyplot(fig)
        plt.close(fig)

    with col2:
        st.header("ğŸ’¬ æ™ºèƒ½å¯¹è¯åŠ©æ‰‹")
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # å¿«æ·æé—®æŒ‰é’®
        st.subheader("ğŸš€ å¿«æ·æé—®")
        quick_questions = [
            "å½“å‰ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å¦‚ä½•ï¼Ÿ",
            "æ£€æµ‹åˆ°çš„æ˜¯ä»€ä¹ˆç±»å‹çš„æ•…éšœï¼Ÿ", 
            "è¯·ç”Ÿæˆè¯¦ç»†çš„è¯Šæ–­æŠ¥å‘Š",
            "ç³»ç»Ÿç¨³å®šæ€§æ€ä¹ˆæ ·ï¼Ÿ",
            "æ ¹æ®å½“å‰æƒ…å†µåˆ›å»ºç»´æŠ¤å·¥å•",
            "è¿™ä¸ªé£é™©éœ€è¦ç«‹å³å¤„ç†å—ï¼Ÿ"
        ]
        
        cols = st.columns(2)
        for i, question in enumerate(quick_questions):
            with cols[i % 2]:
                if st.button(question, key=f"quick_{i}", use_container_width=True):
                    # å¤„ç†å¿«æ·æé—®
                    st.session_state.messages.append({"role": "user", "content": question})
                    with st.chat_message("user"):
                        st.markdown(question)

                    with st.chat_message("assistant"):
                        with st.spinner("æµ·å®‰æ­£åœ¨æ€è€ƒ..."):
                            response = gemini_agent_response(
                                question, 
                                system_status, 
                                st.session_state.messages
                            )
                        
                        # æµç•…è¾“å‡ºæ•ˆæœ
                        message_placeholder = st.empty()
                        full_response = ""
                        for char in response:
                            full_response += char
                            message_placeholder.markdown(full_response + "â–Œ")
                            time.sleep(0.01)
                        message_placeholder.markdown(full_response)
                    
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    st.rerun()
        
        # èŠå¤©è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("æµ·å®‰æ­£åœ¨åˆ†æ..."):
                    response = gemini_agent_response(
                        prompt, 
                        system_status, 
                        st.session_state.messages
                    )
                
                message_placeholder = st.empty()
                full_response = ""
                for char in response:
                    full_response += char
                    message_placeholder.markdown(full_response + "â–Œ")
                    time.sleep(0.005)  # æ›´å¿«çš„è¾“å‡ºé€Ÿåº¦
                message_placeholder.markdown(full_response)
                
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.rerun()

if __name__ == "__main__":
    main()
