import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import time
from google import genai
from google.genai import types
# å¼•å…¥ pydantic ç”¨äºå®šä¹‰å·¥å…·å‡½æ•°çš„è¾“å…¥ç»“æ„
from pydantic import BaseModel, Field

# --- 0. ç¯å¢ƒå’Œå·¥å…·å®šä¹‰ ---

# æ¨èä½¿ç”¨ pydantic å®šä¹‰è¾“å…¥ç»“æ„ï¼Œå¸®åŠ© LLM å‡†ç¡®ç†è§£å‚æ•°
class ReportInput(BaseModel):
    """ç”¨äºç”Ÿæˆè¯¦ç»†æ•…éšœè¯Šæ–­æŠ¥å‘Šçš„å·¥å…·"""
    # LLM ä¼šæ ¹æ®è¿™ä¸ªæè¿°æ¥å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
    fault_id: str = Field(description="å½“å‰æ•…éšœäº‹ä»¶çš„å”¯ä¸€æ ‡è¯†IDï¼Œä¾‹å¦‚ï¼š'EVENT-20251028-001'")
    severity: str = Field(description="æ•…éšœçš„ä¸¥é‡ç¨‹åº¦ï¼Œä¾‹å¦‚ï¼š'ä¸€çº§é¢„è­¦'æˆ–'äºŒçº§é¢„è­¦'")

class StabilityInput(BaseModel):
    """ç”¨äºæŸ¥è¯¢èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒå’Œèˆ¹å²¸ååŒé€šä¿¡é“¾è·¯çš„å®æ—¶çŠ¶æ€å’Œè´Ÿè½½ç‡"""
    # è¿™ä¸ªå·¥å…·ä¸éœ€è¦å‚æ•°ï¼Œä½†å®šä¹‰ Pydantic ä»æœ‰åŠ©äºæ–‡æ¡£åŒ–

# --- å®šä¹‰å·¥å…·å‡½æ•° ---
# å·¥å…·å‡½æ•°å¿…é¡»è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½œä¸º LLM çš„ä¸Šä¸‹æ–‡è¾“å…¥
def generate_diagnostic_report(fault_id: str = "CURRENT-FAULT", severity: str = "Level 2") -> str:
    """
    å®é™…è°ƒç”¨åç«¯æœåŠ¡ï¼Œæ ¹æ®DLæ¨¡å‹ç»“æœå’ŒLLMè¯Šæ–­ç”Ÿæˆæ ¼å¼åŒ–PDFæŠ¥å‘Šã€‚
    """
    # æ¨¡æ‹Ÿå®é™…æ“ä½œï¼Œè¿”å›æ“ä½œç»“æœ
    return f"ã€å·¥å…·è°ƒç”¨æˆåŠŸã€‘å·²è‡ªåŠ¨ç”Ÿæˆå¹¶å‘é€ã€Š{severity} çº§è¯Šæ–­æŠ¥å‘Š ({fault_id})ã€‹è‡³æ‚¨çš„è¿ç»´ç»ˆç«¯ã€‚åŒæ—¶å·²æ ¹æ® RAG çŸ¥è¯†åº“ï¼Œå»ºè®®æ£€æŸ¥ç”µç¼†å›ºå®šä»¶ã€‚"

def check_system_stability() -> str:
    """
    æ­¤å·¥å…·ç”¨äºæŸ¥è¯¢èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒå’Œèˆ¹å²¸ååŒé€šä¿¡é“¾è·¯çš„å®æ—¶çŠ¶æ€å’Œè´Ÿè½½ç‡ã€‚
    """
    # æ¨¡æ‹ŸæŸ¥è¯¢ç³»ç»ŸçŠ¶æ€ï¼Œè¿”å›å®æ—¶ç»“æœ
    return (
        "**ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š**ï¼šèˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒè´Ÿè½½ç‡ç¨³å®šåœ¨38%ï¼Œæ¨¡å‹æ¨ç†å»¶è¿Ÿåœ¨15msä»¥å†…ï¼Œ**ç¨³å®šæ€§è‰¯å¥½**ã€‚"
        "èˆ¹å²¸ååŒé€šä¿¡é“¾è·¯å»¶è¿Ÿä½äº50msï¼ŒçŠ¶æ€ç¨³å®šã€‚"
    )

# å°†å·¥å…·å‡½æ•°æ‰“åŒ…æˆå­—å…¸
AVAILABLE_TOOLS = {
    "generate_diagnostic_report": generate_diagnostic_report,
    "check_system_stability": check_system_stability,
}


# --- 1. æ•°æ®æ¨¡æ‹Ÿï¼ˆä¸å˜ï¼‰---
def simulate_current_data(t, is_fault, prediction_mode=False):
    base_frequency = 50
    time_series = np.linspace(0, 1 / base_frequency, t)
    current = 10 * np.sin(2 * np.pi * base_frequency * time_series) + np.random.normal(0, 0.1, t)

    if is_fault:
        high_freq_noise = np.sin(2 * np.pi * 5000 * time_series) * 0.5 * np.random.rand(t)
        arc_pulse = np.maximum(0, np.sin(2 * np.pi * 50 * time_series) - 0.5) * 5
        current += high_freq_noise * arc_pulse

    if prediction_mode:
        current += 0.5 * np.exp(-time_series * 5) * np.sin(2 * np.pi * 200 * time_series)

    return time_series * 1000, current


# --- 2. æ¨¡å‹æ¨ç†æ¨¡æ‹Ÿï¼ˆä¸å˜ï¼‰---
def dl_model_inference(data):
    if data.max() > 14 or data.min() < -14:
        return "äºŒçº§é¢„è­¦ (æ•…éšœç¡®è®¤)", 97.5
    elif data.max() > 12 or data.min() < -12:
        return "ä¸€çº§é¢„è­¦ (é¢„æµ‹é£é™©)", 75.0
    else:
        return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 5.0


# --- 3. æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘ï¼ˆä½¿ç”¨ Gemini API æ›¿æ¢ï¼‰---
@st.cache_resource # ç¼“å­˜å®¢æˆ·ç«¯ï¼Œé¿å…æ¯æ¬¡è¿è¡Œéƒ½é‡æ–°åˆå§‹åŒ–
def get_gemini_client():
    """å®‰å…¨åœ°è·å– Gemini å®¢æˆ·ç«¯"""
    try:
        GEMINI_API_KEY = st.secrets["gemini_api_key"]
        return genai.Client(api_key=GEMINI_API_KEY)
    except KeyError:
        st.error("æ— æ³•æ‰¾åˆ° Gemini API å¯†é’¥ã€‚è¯·åœ¨ Streamlit Cloud çš„ Secrets ä¸­é…ç½® 'gemini_api_key'ã€‚")
        st.stop()
    except Exception as e:
        st.error(f"åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        st.stop()

def gemini_agent_response(user_query: str, current_status: str):
    client = get_gemini_client()
    
    # æ¨¡æ‹Ÿ RAG çŸ¥è¯†åº“å’Œ Informer é¢„æµ‹ç»“æœï¼ˆä½œä¸ºç³»ç»ŸæŒ‡ä»¤çš„è¾“å…¥ï¼‰
    RAG_AND_INFORME_CONTEXT = (
        "ã€ä¸“ä¸šèƒŒæ™¯çŸ¥è¯†ã€‘:\n"
        "1. **é¢„æµ‹æ¨¡å‹ (Informer)**ï¼šå¦‚æœçŠ¶æ€æ˜¯ä¸€çº§é¢„è­¦ï¼Œé¢„æµ‹æ³¢å½¢å°†å‘ˆç°æŒç»­æ¶åŒ–çš„ä¸è§„åˆ™é«˜é¢‘éœ‡è¡ã€‚\n"
        "2. **è¯Šæ–­çŸ¥è¯†**ï¼šæ•…éšœä¸»è¦åŸå› ä¸ºé«˜æŒ¯åŠ¨åŒºåŸŸçš„ç”µç¼†å›ºå®šä»¶è€åŒ–æ¾åŠ¨ã€‚\n"
        "3. **è§„èŒƒæŸ¥è¯¢**ï¼šèˆ¹çº§ç¤¾è§„èŒƒ[XX-2023]ç¬¬5.4.1æ¡è¦æ±‚ï¼šå¯¹äºé«˜æŒ¯åŠ¨åŒºåŸŸçš„ç”µæ°”è¿æ¥ç‚¹ï¼Œåº”æ¯å­£åº¦è¿›è¡Œé¢„é˜²æ€§æ£€æŸ¥ã€‚\n"
    )

    system_instruction = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½ä½“ï¼Œè¯·ç”¨ä¸¥è°¨ã€ä¸“ä¸šçš„è¯­æ°”å›å¤ç”¨æˆ·çš„æé—®ã€‚"
        "ä½ çš„å›å¤å¿…é¡»åŸºäºDLæ¨¡å‹çš„å®æ—¶çŠ¶æ€å’Œæä¾›çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œè¯Šæ–­å’Œå›å¤ã€‚"
        "è¯·å°½å¯èƒ½åˆ©ç”¨æä¾›çš„ Tool-Calling èƒ½åŠ›æ¥æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼Œä¸è¦è‡ªå·±ç¼–é€ æŠ¥å‘Šæˆ–ç³»ç»ŸçŠ¶æ€ã€‚"
        f"å½“å‰æ¨¡å‹æ£€æµ‹çŠ¶æ€ä¸ºï¼š{current_status}ã€‚"
    )
    
    # æ„å»ºå†å²æ¶ˆæ¯ (æ­¤å¤„çœç•¥äº†å®Œæ•´çš„å†å²æ¶ˆæ¯æ„å»ºï¼Œä»…å‘é€å½“å‰ Prompt)
    
    # åˆå¹¶ RAG çŸ¥è¯†å’Œç”¨æˆ·æé—®
    full_prompt = RAG_AND_INFORME_CONTEXT + "\n\nç”¨æˆ·æé—®ï¼š" + user_query

    try:
        # é…ç½®å·¥å…·å’Œç³»ç»ŸæŒ‡ä»¤
        config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            tools=list(AVAILABLE_TOOLS.values()),
        )
        
        # ç¬¬ä¸€æ¬¡ API è°ƒç”¨
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=full_prompt,
            config=config,
        )
        
        # 1. å¤„ç† Tool-Calling
        if response.function_calls:
            # Gemini å†³å®šè°ƒç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå·¥å…·
            for function_call in response.function_calls:
                tool_name = function_call.name
                tool_args = dict(function_call.args)
                
                if tool_name in AVAILABLE_TOOLS:
                    # æ‰§è¡Œæœ¬åœ°å·¥å…·å‡½æ•°
                    tool_result = AVAILABLE_TOOLS[tool_name](**tool_args)
                    
                    # ç¬¬äºŒæ¬¡ API è°ƒç”¨ï¼šå°†å·¥å…·æ‰§è¡Œç»“æœåé¦ˆç»™ LLM
                    response_after_tool = client.models.generate_content(
                        model='gemini-2.5-flash',
                        contents=[
                            types.Content(role="user", parts=[types.Part.from_text(full_prompt)]),
                            types.Content(role="model", parts=[types.Part.from_function_call(function_call)]),
                            types.Content(role="tool", parts=[types.Part.from_text(tool_result)]),
                        ],
                        config=types.GenerateContentConfig(system_instruction=system_instruction),
                    )
                    return response_after_tool.text # è¿”å› LLM åŸºäºå·¥å…·ç»“æœçš„æœ€ç»ˆå›å¤
                
        # 2. è¿”å› LLM çš„æ ‡å‡†æ–‡æœ¬å›å¤
        return response.text

    except Exception as e:
        # æ•è· API é”™è¯¯ï¼Œè¿”å›å‹å¥½æç¤º
        return f"æ™ºèƒ½ä½“ API è°ƒç”¨å¤±è´¥ã€‚è¯·æ£€æŸ¥ Gemini API å¯†é’¥æ˜¯å¦æœ‰æ•ˆæˆ–ç½‘ç»œè¿æ¥ã€‚é”™è¯¯ä¿¡æ¯: {e}"


# --- 4. ä¸»ç•Œé¢ï¼ˆåªéœ€è¦æ›¿æ¢è°ƒç”¨å‡½æ•°ï¼‰---
def main():
    st.set_page_config(layout="wide", page_title="èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å°")
    st.title("ğŸš¢ èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å° (æ¼”ç¤ºåŸå‹)")

    # åˆå§‹åŒ–çŠ¶æ€
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'is_fault_active' not in st.session_state:
        st.session_state.is_fault_active = False
    if 't_start' not in st.session_state:
        st.session_state.t_start = time.time()
    
    # å°è¯•åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œæ£€æŸ¥å¯†é’¥
    get_gemini_client() 

    col1, col2 = st.columns([3, 2])

    with col1:
        st.header("å®æ—¶ç›‘æµ‹ä¸é¢„è­¦ Dashboard (èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—æ¨¡æ‹Ÿ)")

        # æ•…éšœåˆ‡æ¢æŒ‰é’®ï¼ˆç”¨å›è°ƒæ›´æ–°çŠ¶æ€ï¼‰
        def toggle_fault():
            st.session_state.is_fault_active = not st.session_state.is_fault_active
            st.session_state.t_start = time.time()
            st.toast(f"æ•…éšœçŠ¶æ€å·²åˆ‡æ¢è‡³: {'æ•…éšœæ¨¡å¼' if st.session_state.is_fault_active else 'æ­£å¸¸æ¨¡å¼'}")
            
        st.button(
            "ğŸ”´ æ¨¡æ‹Ÿæ•…éšœç”µå¼§å‘ç”Ÿ / ğŸŸ¢ æ¢å¤æ­£å¸¸è¿è¡Œ",
            on_click=toggle_fault
        )

        # ç”Ÿæˆå®æ—¶æ•°æ®
        t_series, current_data = simulate_current_data(
            t=2000,
            is_fault=st.session_state.is_fault_active,
            prediction_mode=(time.time() - st.session_state.t_start < 20 and not st.session_state.is_fault_active)
        )
        status_text, confidence = dl_model_inference(current_data)

        # æ˜¾ç¤ºçŠ¶æ€
        color = "green"
        if "ä¸€çº§é¢„è­¦" in status_text:
            color = "orange"
        elif "äºŒçº§é¢„è­¦" in status_text:
            color = "red"
        st.markdown(
            f"**æ¨¡å‹æ£€æµ‹çŠ¶æ€:** <span style='color:{color}; font-size: 20px;'>{status_text}</span> | **ç½®ä¿¡åº¦:** {confidence:.1f}%",
            unsafe_allow_html=True
        )

        # ç»˜åˆ¶æ³¢å½¢å›¾
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(t_series, current_data, label='ç”µæµæ³¢å½¢ (A)', color=color)
        ax.set_title("å®æ—¶ç”µæµæ³¢å½¢ç›‘æµ‹ (èˆ¹ç«¯)")
        ax.set_xlabel("æ—¶é—´ (ms)")
        ax.set_ylabel("ç”µæµ (A)")
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.set_ylim(-15, 15)
        st.pyplot(fig)
        plt.close(fig)

    with col2:
        st.header("æ™ºèƒ½ä½“äº¤äº’ä¸­å¿ƒ (å²¸åŸºè¿ç»´ä¸­å¿ƒæ¨¡æ‹Ÿ)")

        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # é¢„è®¾å¯¹è¯å¼•å¯¼
        st.subheader("ğŸ’¡ é¢„è®¾æ¼”ç¤ºå¯¹è¯ (è¯·æ‰‹åŠ¨è¾“å…¥)")
        st.info("1. **å‰ç»é¢„è­¦**: è¯·é—®å½“å‰æ³¢å½¢èµ°åŠ¿æ˜¯å¦æ­£å¸¸ï¼Ÿæœ‰æ— æ½œåœ¨çš„ç”µå¼§é£é™©ï¼Ÿ")
        st.info("2. **è¯Šæ–­æŸ¥è¯¢**: è¯·é—®å¦‚ä½•æŸ¥è¯¢æ•…éšœç”µå¼§å‘ç”Ÿçš„æ ¹æœ¬åŸå› ï¼Œä»¥åŠèˆ¹çº§ç¤¾çš„ç»´æŠ¤è¦æ±‚ï¼Ÿ")
        st.info("3. **ç³»ç»ŸçŠ¶æ€**: è¯·å‘ŠçŸ¥æˆ‘è¾¹ç¼˜è®¡ç®—å•å…ƒçš„è´Ÿè½½ç‡å’Œç³»ç»Ÿç¨³å®šæ€§å¦‚ä½•ï¼Ÿ")

        # èŠå¤©è¾“å…¥å¤„ç†
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # ç”Ÿæˆæ™ºèƒ½ä½“å“åº”
            with st.chat_message("assistant"):
                current_status = status_text
                
                # *** æ›¿æ¢ä¸ºæ–°çš„ Gemini API è°ƒç”¨å‡½æ•° ***
                response = gemini_agent_response(prompt, current_status)
                
                # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                full_response = ""
                message_placeholder = st.empty()
                for chunk in response.split():
                    full_response += chunk + " "
                    time.sleep(0.01) # ç•¥å¾®å‡å°‘å»¶è¿Ÿï¼Œå› ä¸º API è°ƒç”¨å·²æœ‰å»¶è¿Ÿ
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
                
            # ä¿å­˜æ™ºèƒ½ä½“å“åº”
            st.session_state.messages.append({"role": "assistant", "content": response})
            # å¼ºåˆ¶åˆ·æ–°ä»¥æ˜¾ç¤ºæœ€æ–°çš„èŠå¤©è®°å½•å’ŒæŒ‰é’®çŠ¶æ€
            st.rerun() 


if __name__ == "__main__":
    main()
