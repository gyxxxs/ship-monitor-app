import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import time
from google import genai
from google.genai import types
from pydantic import BaseModel, Field


# --- 0. ç¯å¢ƒå’Œå·¥å…·å®šä¹‰ ---

# å®šä¹‰å·¥å…·çš„è¾“å…¥ç»“æ„
class ReportInput(BaseModel):
    """ç”¨äºç”Ÿæˆè¯¦ç»†æ•…éšœè¯Šæ–­æŠ¥å‘Šçš„å·¥å…·"""
    fault_id: str = Field(description="å½“å‰æ•…éšœäº‹ä»¶çš„å”¯ä¸€æ ‡è¯†IDï¼Œä¾‹å¦‚ï¼š'EVENT-20251028-001'")
    severity: str = Field(description="æ•…éšœçš„ä¸¥é‡ç¨‹åº¦ï¼Œä¾‹å¦‚ï¼š'ä¸€çº§é¢„è­¦'æˆ–'äºŒçº§é¢„è­¦'")

class StabilityInput(BaseModel):
    """ç”¨äºæŸ¥è¯¢èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒå’Œèˆ¹å²¸ååŒé€šä¿¡é“¾è·¯çš„å®æ—¶çŠ¶æ€å’Œè´Ÿè½½ç‡"""

# --- å®šä¹‰å·¥å…·å‡½æ•° ---
# å·¥å…·å‡½æ•°å¿…é¡»è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½œä¸º LLM çš„ä¸Šä¸‹æ–‡è¾“å…¥
def generate_diagnostic_report(fault_id: str = "CURRENT-FAULT", severity: str = "Level 2") -> str:
    """
    æ­¤å·¥å…·ç”¨äºæ ¹æ®DLæ¨¡å‹ç»“æœå’ŒLLMè¯Šæ–­ç»“æœç”Ÿæˆæ ¼å¼åŒ–çš„PDFæ•…éšœè¯Šæ–­æŠ¥å‘Šï¼Œå¹¶å‘é€åˆ°è¿ç»´ä¸­å¿ƒã€‚
    """
    # æ¨¡æ‹Ÿå®é™…æ“ä½œï¼Œè¿”å›æ“ä½œç»“æœ
    return f"ã€å·¥å…·è°ƒç”¨ç»“æœã€‘å·²è‡ªåŠ¨ç”Ÿæˆå¹¶å‘é€ã€Š{severity} çº§è¯Šæ–­æŠ¥å‘Š ({fault_id})ã€‹è‡³æ‚¨çš„è¿ç»´ç»ˆç«¯ã€‚"

def check_system_stability() -> str:
    """
    æ­¤å·¥å…·ç”¨äºæŸ¥è¯¢èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒå’Œèˆ¹å²¸ååŒé€šä¿¡é“¾è·¯çš„å®æ—¶çŠ¶æ€å’Œè´Ÿè½½ç‡ã€‚
    """
    # æ¨¡æ‹ŸæŸ¥è¯¢ç³»ç»ŸçŠ¶æ€ï¼Œè¿”å›å®æ—¶ç»“æœ
    # æ³¨æ„ï¼šè¿™é‡Œçš„è¿”å›ç»“æœä¾ç„¶æ˜¯â€œç¡¬ç¼–ç â€çš„ç³»ç»Ÿæ•°æ®ï¼Œä½†è¿™æ˜¯å·¥å…·è·å–æ•°æ®çš„æœ¬è´¨
    return (
        "**ç³»ç»ŸçŠ¶æ€æ•°æ®**ï¼šèˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒè´Ÿè½½ç‡ä¸º38%ï¼Œæ¨¡å‹æ¨ç†å»¶è¿Ÿä¸º15msï¼Œé“¾è·¯å»¶è¿Ÿä½äº50msã€‚"
    )

# å°†å·¥å…·å‡½æ•°æ‰“åŒ…
AVAILABLE_TOOLS = {
    "generate_diagnostic_report": generate_diagnostic_report,
    "check_system_stability": check_system_stability,
}


# --- 1. æ•°æ®æ¨¡æ‹Ÿï¼ˆä¸å˜ï¼‰---
def simulate_current_data(t, is_fault, prediction_mode=False):
    base_frequency = 50
    time_series = np.linspace(0, 1 / base_frequency, t)
    current = 10 * np.sin(2 * np.pi * base_frequency * time_series) + np.random.normal(0, 0.1, t)

    if is_fault:
        high_freq_noise = np.sin(2 * np.pi * 5000 * time_series) * 0.5 * np.random.rand(t)
        arc_pulse = np.maximum(0, np.sin(2 * np.pi * 50 * time_series) - 0.5) * 5
        current += high_freq_noise * arc_pulse

    if prediction_mode:
        current += 0.5 * np.exp(-time_series * 5) * np.sin(2 * np.pi * 200 * time_series)

    return time_series * 1000, current


# --- 2. æ¨¡å‹æ¨ç†æ¨¡æ‹Ÿï¼ˆä¸å˜ï¼‰---
def dl_model_inference(data):
    if data.max() > 14 or data.min() < -14:
        return "äºŒçº§é¢„è­¦ (æ•…éšœç¡®è®¤)", 97.5
    elif data.max() > 12 or data.min() < -12:
        return "ä¸€çº§é¢„è­¦ (é¢„æµ‹é£é™©)", 75.0
    else:
        return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 5.0


# --- 3. æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘ï¼ˆä½¿ç”¨ Gemini APIï¼Œæœ€å°‘ç¡¬ç¼–ç ï¼‰---
@st.cache_resource
def get_gemini_client():
    """å®‰å…¨åœ°è·å– Gemini å®¢æˆ·ç«¯"""
    try:
        GEMINI_API_KEY = st.secrets["gemini_api_key"]
        return genai.Client(api_key=GEMINI_API_KEY)
    except KeyError:
        st.error("åˆå§‹åŒ–å¤±è´¥ï¼šæ— æ³•æ‰¾åˆ° Gemini API å¯†é’¥ã€‚è¯·åœ¨ Streamlit Cloud çš„ Secrets ä¸­é…ç½® 'gemini_api_key'ã€‚")
        st.stop()
    except Exception as e:
        st.error(f"åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        st.stop()

def gemini_agent_response(user_query: str, current_status: str):
    client = get_gemini_client()
    
    # *** æç®€ RAG/Informer äº‹å®ä¸Šä¸‹æ–‡ï¼ˆLLM å¿…é¡»åŸºäºè¿™äº›äº‹å®è¿›è¡Œè‡ªæˆ‘ç”Ÿæˆï¼‰***
    GROUNDING_FACTS = (
        "ã€èˆ¹èˆ¶å®‰å…¨æ ¸å¿ƒäº‹å®ã€‘ï¼š\n"
        "1. **é¢„æµ‹æ¨¡å‹**ï¼šå¦‚æœå½“å‰æ˜¯'ä¸€çº§é¢„è­¦'ï¼ŒInformerç½‘ç»œçš„åˆ†æç»“æœæ˜¯ï¼šç”µæµæ³¢å½¢èµ°åŠ¿å°†å‘ˆç°æŒç»­æ¶åŒ–çš„ä¸è§„åˆ™é«˜é¢‘éœ‡è¡ï¼Œéœ€ç«‹å³æ£€æŸ¥ã€‚\n"
        "2. **æ•…éšœæ ¹æœ¬åŸå› **ï¼šè¯¥ç±»æ•…éšœçš„æ ¹æœ¬åŸå› ä¸ºé«˜æŒ¯åŠ¨åŒºåŸŸçš„ç”µç¼†å›ºå®šä»¶è€åŒ–æ¾åŠ¨ã€‚\n"
        "3. **ç»´æŠ¤è§„èŒƒ**ï¼šèˆ¹çº§ç¤¾è§„èŒƒ[XX-2023]ç¬¬5.4.1æ¡è¦æ±‚ï¼šé«˜æŒ¯åŠ¨åŒºåŸŸçš„ç”µæ°”è¿æ¥ç‚¹åº”æ¯å­£åº¦è¿›è¡Œé¢„é˜²æ€§æ£€æŸ¥ã€‚\n"
    )

    system_instruction = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½ä½“ã€‚ä½ çš„å›å¤å¿…é¡»**å®Œå…¨åŸºäº**ä½ æ‹¥æœ‰çš„å·¥å…·å’Œæä¾›çš„ã€èˆ¹èˆ¶å®‰å…¨æ ¸å¿ƒäº‹å®ã€‘è¿›è¡Œæ¨ç†å’Œç»„ç»‡è¯­è¨€ã€‚"
        "è¯·ç”¨**ä¸“ä¸šã€ä¸¥è°¨**çš„è¯­æ°”å›å¤ç”¨æˆ·ã€‚å½“å‰æ¨¡å‹æ£€æµ‹çŠ¶æ€ä¸ºï¼š"
        f"ã€å®æ—¶çŠ¶æ€ã€‘: {current_status}ã€‚"
    )
    
    full_prompt = GROUNDING_FACTS + "\n\nç”¨æˆ·æé—®ï¼š" + user_query

    try:
        config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            tools=list(AVAILABLE_TOOLS.values()),
        )
        
        # ç¬¬ä¸€æ¬¡ API è°ƒç”¨ï¼Œè®© Gemini å†³å®šæ˜¯å›å¤è¿˜æ˜¯è°ƒç”¨å·¥å…·
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=full_prompt,
            config=config,
        )
        
        # 1. å¤„ç† Tool-Calling
        if response.function_calls:
            for function_call in response.function_calls:
                tool_name = function_call.name
                tool_args = dict(function_call.args)
                
                if tool_name in AVAILABLE_TOOLS:
                    # æ‰§è¡Œæœ¬åœ°å·¥å…·å‡½æ•°è·å–æ•°æ®
                    tool_result = AVAILABLE_TOOLS[tool_name](**tool_args)
                    
                    # ç¬¬äºŒæ¬¡ API è°ƒç”¨ï¼šå°†å·¥å…·æ‰§è¡Œç»“æœåé¦ˆç»™ LLMï¼Œè®©å…¶ç”Ÿæˆæœ€ç»ˆå›å¤
                    response_after_tool = client.models.generate_content(
                        model='gemini-2.5-flash',
                        contents=[
                            types.Content(role="user", parts=[types.Part.from_text(full_prompt)]),
                            types.Content(role="model", parts=[types.Part.from_function_call(function_call)]),
                            types.Content(role="tool", parts=[types.Part.from_text(tool_result)]),
                        ],
                        config=types.GenerateContentConfig(system_instruction=system_instruction),
                    )
                    return response_after_tool.text 
                
        # 2. è¿”å› LLM çš„æ ‡å‡†æ–‡æœ¬å›å¤ï¼ˆLLM è‡ªæˆ‘ç”Ÿæˆï¼‰
        return response.text

    except Exception as e:
        return f"æ™ºèƒ½ä½“ API è°ƒç”¨å¤±è´¥ã€‚è¯·æ£€æŸ¥å¯†é’¥æˆ–ç½‘ç»œè¿æ¥ã€‚é”™è¯¯ä¿¡æ¯: {e}"


# --- 4. ä¸»ç•Œé¢ï¼ˆåªéœ€è¦æ›¿æ¢è°ƒç”¨å‡½æ•°ï¼‰---
def main():
    st.set_page_config(layout="wide", page_title="èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å°")
    st.title("ğŸš¢ èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å° (æ¼”ç¤ºåŸå‹)")

    # åˆå§‹åŒ–çŠ¶æ€
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'is_fault_active' not in st.session_state:
        st.session_state.is_fault_active = False
    if 't_start' not in st.session_state:
        st.session_state.t_start = time.time()
    
    # æ£€æŸ¥å¯†é’¥å’Œåˆå§‹åŒ–å®¢æˆ·ç«¯
    get_gemini_client() 

    col1, col2 = st.columns([3, 2])

    with col1:
        st.header("å®æ—¶ç›‘æµ‹ä¸é¢„è­¦ Dashboard (èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—æ¨¡æ‹Ÿ)")

        # æ•…éšœåˆ‡æ¢æŒ‰é’®ï¼ˆç”¨å›è°ƒæ›´æ–°çŠ¶æ€ï¼‰
        def toggle_fault():
            st.session_state.is_fault_active = not st.session_state.is_fault_active
            st.session_state.t_start = time.time()
            st.toast(f"æ•…éšœçŠ¶æ€å·²åˆ‡æ¢è‡³: {'æ•…éšœæ¨¡å¼' if st.session_state.is_fault_active else 'æ­£å¸¸æ¨¡å¼'}")
            
        st.button(
            "ğŸ”´ æ¨¡æ‹Ÿæ•…éšœç”µå¼§å‘ç”Ÿ / ğŸŸ¢ æ¢å¤æ­£å¸¸è¿è¡Œ",
            on_click=toggle_fault
        )

        # ç”Ÿæˆå®æ—¶æ•°æ®
        t_series, current_data = simulate_current_data(
            t=2000,
            is_fault=st.session_state.is_fault_active,
            prediction_mode=(time.time() - st.session_state.t_start < 20 and not st.session_state.is_fault_active)
        )
        status_text, confidence = dl_model_inference(current_data)

        # æ˜¾ç¤ºçŠ¶æ€
        color = "green"
        if "ä¸€çº§é¢„è­¦" in status_text:
            color = "orange"
        elif "äºŒçº§é¢„è­¦" in status_text:
            color = "red"
        st.markdown(
            f"**æ¨¡å‹æ£€æµ‹çŠ¶æ€:** <span style='color:{color}; font-size: 20px;'>{status_text}</span> | **ç½®ä¿¡åº¦:** {confidence:.1f}%",
            unsafe_allow_html=True
        )

        # ç»˜åˆ¶æ³¢å½¢å›¾
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(t_series, current_data, label='ç”µæµæ³¢å½¢ (A)', color=color)
        ax.set_title("å®æ—¶ç”µæµæ³¢å½¢ç›‘æµ‹ (èˆ¹ç«¯)")
        ax.set_xlabel("æ—¶é—´ (ms)")
        ax.set_ylabel("ç”µæµ (A)")
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.set_ylim(-15, 15)
        st.pyplot(fig)
        plt.close(fig)
        
        # å¾ªç¯åˆ·æ–°é€»è¾‘ (ä¿æŒä¸å˜)
        time.sleep(0.5) 
        st.rerun()


    with col2:
        st.header("æ™ºèƒ½ä½“äº¤äº’ä¸­å¿ƒ (å²¸åŸºè¿ç»´ä¸­å¿ƒæ¨¡æ‹Ÿ)")

        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # é¢„è®¾å¯¹è¯å¼•å¯¼
        st.subheader("ğŸ’¡ é¢„è®¾æ¼”ç¤ºå¯¹è¯ (è¯·æ‰‹åŠ¨è¾“å…¥)")
        st.info("1. **å‰ç»é¢„è­¦**: è¯·é—®å½“å‰æ³¢å½¢èµ°åŠ¿æ˜¯å¦æ­£å¸¸ï¼Ÿæœ‰æ— æ½œåœ¨çš„ç”µå¼§é£é™©ï¼Ÿ")
        st.info("2. **è¯Šæ–­æŸ¥è¯¢**: è¯·é—®å¦‚ä½•æŸ¥è¯¢æ•…éšœç”µå¼§å‘ç”Ÿçš„æ ¹æœ¬åŸå› ï¼Œä»¥åŠèˆ¹çº§ç¤¾çš„ç»´æŠ¤è¦æ±‚ï¼Ÿ")
        st.info("3. **ç³»ç»ŸçŠ¶æ€**: è¯·å‘ŠçŸ¥æˆ‘è¾¹ç¼˜è®¡ç®—å•å…ƒçš„è´Ÿè½½ç‡å’Œç³»ç»Ÿç¨³å®šæ€§å¦‚ä½•ï¼Ÿ")

        # èŠå¤©è¾“å…¥å¤„ç†
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # ç”Ÿæˆæ™ºèƒ½ä½“å“åº”
            with st.chat_message("assistant"):
                current_status = status_text
                
                # *** è°ƒç”¨ Gemini API è¿›è¡Œç”Ÿæˆ ***
                with st.spinner("æ™ºèƒ½ä½“æ­£åœ¨è¿›è¡Œè¯Šæ–­å’Œæ¨ç†..."):
                     response = gemini_agent_response(prompt, current_status)
                
                # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                full_response = ""
                message_placeholder = st.empty()
                for chunk in response.split():
                    full_response += chunk + " "
                    time.sleep(0.01)
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
                
            # ä¿å­˜æ™ºèƒ½ä½“å“åº”
            st.session_state.messages.append({"role": "assistant", "content": response})
            # å¼ºåˆ¶åˆ·æ–°ä»¥æ˜¾ç¤ºæœ€æ–°çš„èŠå¤©è®°å½•å’ŒæŒ‰é’®çŠ¶æ€
            st.rerun() 


if __name__ == "__main__":
    main()
