import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import time
from google import genai
from google.genai import types
from pydantic import BaseModel, Field

# --- 0. ç¯å¢ƒå’Œå·¥å…·å®šä¹‰ ---

# å®šä¹‰å·¥å…·çš„è¾“å…¥ç»“æ„
class ReportInput(BaseModel):
    """ç”¨äºç”Ÿæˆè¯¦ç»†æ•…éšœè¯Šæ–­æŠ¥å‘Šçš„å·¥å…·"""
    fault_id: str = Field(description="å½“å‰æ•…éšœäº‹ä»¶çš„å”¯ä¸€æ ‡è¯†IDï¼Œä¾‹å¦‚ï¼š'EVENT-20251028-001'")
    severity: str = Field(description="æ•…éšœçš„ä¸¥é‡ç¨‹åº¦ï¼Œä¾‹å¦‚ï¼š'ä¸€çº§é¢„è­¦'æˆ–'äºŒçº§é¢„è­¦'")

class StabilityInput(BaseModel):
    """ç”¨äºæŸ¥è¯¢èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒå’Œèˆ¹å²¸ååŒé€šä¿¡é“¾è·¯çš„å®æ—¶çŠ¶æ€å’Œè´Ÿè½½ç‡"""
    # æ­¤å·¥å…·æ— éœ€å‚æ•°ï¼Œä½†å®šä¹‰ Pydantic ç±»æœ‰åŠ©äºæ–‡æ¡£åŒ–

# --- å®šä¹‰å·¥å…·å‡½æ•° ---
# å·¥å…·å‡½æ•°è¿”å›çš„ç»“æœæ˜¯ LLM è¿›è¡Œæœ€ç»ˆå›å¤æ—¶çš„ä¸Šä¸‹æ–‡
def generate_diagnostic_report(fault_id: str = "CURRENT-FAULT", severity: str = "Level 2") -> str:
    """
    æ­¤å·¥å…·ç”¨äºæ ¹æ®DLæ¨¡å‹ç»“æœå’ŒLLMè¯Šæ–­ç»“æœç”Ÿæˆæ ¼å¼åŒ–çš„PDFæ•…éšœè¯Šæ–­æŠ¥å‘Šï¼Œå¹¶å‘é€åˆ°è¿ç»´ä¸­å¿ƒã€‚
    """
    # è¿”å›æ“ä½œç»“æœå’Œä¸‹ä¸€æ­¥æŒ‡å¯¼ï¼Œè®© LLM ç»„ç»‡è¯­è¨€
    return f"ã€å·¥å…·è°ƒç”¨ç»“æœã€‘è¯Šæ–­æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆï¼Œç¼–å·ä¸º {fault_id}ï¼Œçº§åˆ«ï¼š{severity}ã€‚å·²å‘é€åˆ°å²¸åŸºè¿ç»´ç³»ç»Ÿï¼Œè¯·åŸºäºæŠ¥å‘Šå†…å®¹ç»™å‡ºä¸‹ä¸€æ­¥ç»´æŠ¤å»ºè®®ã€‚"

def check_system_stability() -> str:
    """
    æ­¤å·¥å…·ç”¨äºæŸ¥è¯¢èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒå’Œèˆ¹å²¸ååŒé€šä¿¡é“¾è·¯çš„å®æ—¶çŠ¶æ€å’Œè´Ÿè½½ç‡ã€‚
    """
    # æ¨¡æ‹Ÿä»ç³»ç»ŸAPIè·å–çš„å®æ—¶æ•°æ®
    return (
        "**ç³»ç»ŸçŠ¶æ€æ•°æ®**ï¼šèˆ¹ç«¯è¾¹ç¼˜è®¡ç®—å•å…ƒè´Ÿè½½ç‡ä¸º38%ï¼Œæ¨¡å‹æ¨ç†å»¶è¿Ÿä¸º15msã€‚èˆ¹å²¸ååŒé€šä¿¡é“¾è·¯å»¶è¿Ÿä½äº50msï¼ŒçŠ¶æ€ç¨³å®šã€‚"
    )

# å°†å·¥å…·å‡½æ•°æ‰“åŒ…
AVAILABLE_TOOLS = {
    "generate_diagnostic_report": generate_diagnostic_report,
    "check_system_stability": check_system_stability,
}


# --- 1. æ•°æ®æ¨¡æ‹Ÿï¼ˆä¸å˜ï¼‰---
def simulate_current_data(t, is_fault, prediction_mode=False):
    base_frequency = 50
    time_series = np.linspace(0, 1 / base_frequency, t)
    current = 10 * np.sin(2 * np.pi * base_frequency * time_series) + np.random.normal(0, 0.1, t)

    if is_fault:
        high_freq_noise = np.sin(2 * np.pi * 5000 * time_series) * 0.5 * np.random.rand(t)
        arc_pulse = np.maximum(0, np.sin(2 * np.pi * 50 * time_series) - 0.5) * 5
        current += high_freq_noise * arc_pulse

    if prediction_mode:
        current += 0.5 * np.exp(-time_series * 5) * np.sin(2 * np.pi * 200 * time_series)

    return time_series * 1000, current


# --- 2. æ¨¡å‹æ¨ç†æ¨¡æ‹Ÿï¼ˆä¸å˜ï¼‰---
def dl_model_inference(data):
    if data.max() > 14 or data.min() < -14:
        return "äºŒçº§é¢„è­¦ (æ•…éšœç¡®è®¤)", 97.5
    elif data.max() > 12 or data.min() < -12:
        return "ä¸€çº§é¢„è­¦ (é¢„æµ‹é£é™©)", 75.0
    else:
        return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 5.0


# --- 3. æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘ï¼ˆå®Œå…¨ä¾é  Gemini è‡ªä¸»åˆæˆï¼‰---
@st.cache_resource
def get_gemini_client():
    """å®‰å…¨åœ°è·å– Gemini å®¢æˆ·ç«¯"""
    try:
        GEMINI_API_KEY = st.secrets["gemini_api_key"]
        return genai.Client(api_key=GEMINI_API_KEY)
    except KeyError:
        st.error("åˆå§‹åŒ–å¤±è´¥ï¼šæ— æ³•æ‰¾åˆ° Gemini API å¯†é’¥ã€‚è¯·åœ¨ Streamlit Cloud çš„ Secrets ä¸­é…ç½® 'gemini_api_key'ã€‚")
        st.stop()
    except Exception as e:
        st.error(f"åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        st.stop()

def gemini_agent_response(user_query: str, current_status: str):
    client = get_gemini_client()
    
    # *** RAG çŸ¥è¯†äº‹å®ï¼šä¸º LLM æä¾›è‡ªä¸»åˆæˆçš„åŸºçŸ³ ***
    GROUNDING_FACTS = (
        "ã€RAGæ£€ç´¢ç»“æœï¼šèˆ¹èˆ¶ç”µæ°”å®‰å…¨çŸ¥è¯†åº“ç²¾è¦ã€‘\n"
        "--- 1. é¢„æµ‹ä¸é¢„è­¦ï¼ˆåŸºäº Informer æ¨¡å‹ï¼‰---\n"
        " - **ä¸€çº§é¢„è­¦ç‰¹å¾**ï¼šç”µæµæ³¢å½¢èµ°åŠ¿å°†å‘ˆç°æŒç»­æ¶åŒ–çš„ä¸è§„åˆ™é«˜é¢‘éœ‡è¡ï¼Œè¿™æ˜¯ç”µå¼§åœ¨æ—©æœŸå‘å±•çš„æ˜ç¡®ä¿¡å·ã€‚\n"
        " - **å¤„ç†å»ºè®®**ï¼šå¦‚æœå½“å‰çŠ¶æ€ä¸º'ä¸€çº§é¢„è­¦'ï¼Œåº”ç«‹å³å¯åŠ¨å¯¹è¯¥å›è·¯çš„æ£€æŸ¥ç¨‹åºï¼Œé¿å…æ•…éšœå‡çº§ã€‚\n"
        "--- 2. æ•…éšœè¯Šæ–­ï¼ˆå†å²ç»éªŒå½’å› ï¼‰---\n"
        " - **æ ¹æœ¬åŸå› **ï¼šè¯¥ç±»ä¸²è”æ•…éšœç”µå¼§ä¸»è¦æºäºé«˜æŒ¯åŠ¨åŒºåŸŸçš„ç”µç¼†è¿æ¥ç‚¹æ¥è§¦ä¸è‰¯ï¼Œå¦‚å›ºå®šä»¶è€åŒ–æ¾åŠ¨ã€‚\n"
        "--- 3. ç»´æŠ¤è§„èŒƒï¼ˆèˆ¹çº§ç¤¾è¦æ±‚ï¼‰---\n"
        " - **è§„èŒƒç¼–å·**ï¼šèˆ¹çº§ç¤¾è§„èŒƒ[XX-2023]ç¬¬5.4.1æ¡ã€‚\n"
        " - **ç»´æŠ¤è¦æ±‚**ï¼šå¯¹äºé«˜æŒ¯åŠ¨åŒºåŸŸçš„å…³é”®ç”µæ°”è¿æ¥ç‚¹ï¼Œå¿…é¡»æ¯å­£åº¦è¿›è¡Œé¢„é˜²æ€§æ£€æŸ¥å’Œç´§å›ºç»´æŠ¤ã€‚\n"
    )

    system_instruction = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½ä½“ï¼Œè´Ÿè´£å®æ—¶ç›‘æµ‹ã€æ•…éšœè¯Šæ–­å’Œå®‰å…¨è§„èŒƒå’¨è¯¢ã€‚"
        "ä½ çš„å›å¤å¿…é¡»**å®Œå…¨åŸºäº**ä½ æ‹¥æœ‰çš„å·¥å…·å’Œæä¾›çš„ã€RAGæ£€ç´¢ç»“æœã€‘ä¸­çš„ä¿¡æ¯è¿›è¡Œæ¨ç†å’Œç»„ç»‡è¯­è¨€ã€‚"
        "è¯·ä½¿ç”¨**ä¸“ä¸šã€ä¸¥è°¨**çš„è¯­æ°”å›å¤ç”¨æˆ·ã€‚å½“å‰æ¨¡å‹æ£€æµ‹çŠ¶æ€ä¸ºï¼š"
        f"ã€å®æ—¶çŠ¶æ€ã€‘: {current_status}ã€‚"
    )
    
    # å°†æ‰€æœ‰ä¿¡æ¯åˆå¹¶ï¼Œä½œä¸º LLM çš„ä¸Šä¸‹æ–‡
    full_prompt = system_instruction + "\n\n" + GROUNDING_FACTS + "\n\nç”¨æˆ·æé—®ï¼š" + user_query

    try:
        config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            tools=list(AVAILABLE_TOOLS.values()),
        )
        
        # ç¬¬ä¸€æ¬¡ API è°ƒç”¨
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=full_prompt,
            config=config,
        )
        
        # 1. å¤„ç† Tool-Calling
        if response.function_calls:
            for function_call in response.function_calls:
                tool_name = function_call.name
                tool_args = dict(function_call.args)
                
                if tool_name in AVAILABLE_TOOLS:
                    # æ‰§è¡Œæœ¬åœ°å·¥å…·å‡½æ•°è·å–æ•°æ®
                    tool_result = AVAILABLE_TOOLS[tool_name](**tool_args)
                    
                    # ç¬¬äºŒæ¬¡ API è°ƒç”¨ï¼šå°†å·¥å…·æ‰§è¡Œç»“æœåé¦ˆç»™ LLMï¼Œè®©å…¶ç”Ÿæˆæœ€ç»ˆå›å¤
                    response_after_tool = client.models.generate_content(
                        model='gemini-2.5-flash',
                        contents=[
                            types.Content(role="user", parts=[types.Part.from_text(full_prompt)]),
                            types.Content(role="model", parts=[types.Part.from_function_call(function_call)]),
                            types.Content(role="tool", parts=[types.Part.from_text(tool_result)]),
                        ],
                        config=types.GenerateContentConfig(system_instruction=system_instruction),
                    )
                    return response_after_tool.text 
                
        # 2. è¿”å› LLM çš„æ ‡å‡†æ–‡æœ¬å›å¤ï¼ˆå®Œå…¨è‡ªä¸»åˆæˆï¼‰
        return response.text

    except Exception as e:
        return f"æ™ºèƒ½ä½“ API è°ƒç”¨å¤±è´¥ã€‚è¯·æ£€æŸ¥å¯†é’¥æˆ–ç½‘ç»œè¿æ¥ã€‚é”™è¯¯ä¿¡æ¯: {e}"


# --- 4. ä¸»ç•Œé¢ï¼ˆåªéœ€è¦æ›¿æ¢è°ƒç”¨å‡½æ•°ï¼‰---
def main():
    st.set_page_config(layout="wide", page_title="èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å°")
    st.title("ğŸš¢ èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å° (æ¼”ç¤ºåŸå‹)")

    # åˆå§‹åŒ–çŠ¶æ€
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'is_fault_active' not in st.session_state:
        st.session_state.is_fault_active = False
    if 't_start' not in st.session_state:
        st.session_state.t_start = time.time()
    
    # æ£€æŸ¥å¯†é’¥å’Œåˆå§‹åŒ–å®¢æˆ·ç«¯
    get_gemini_client() 

    col1, col2 = st.columns([3, 2])

    with col1:
        st.header("å®æ—¶ç›‘æµ‹ä¸é¢„è­¦ Dashboard (èˆ¹ç«¯è¾¹ç¼˜è®¡ç®—æ¨¡æ‹Ÿ)")

        # æ•…éšœåˆ‡æ¢æŒ‰é’®ï¼ˆç”¨å›è°ƒæ›´æ–°çŠ¶æ€ï¼‰
        def toggle_fault():
            st.session_state.is_fault_active = not st.session_state.is_fault_active
            st.session_state.t_start = time.time()
            st.toast(f"æ•…éšœçŠ¶æ€å·²åˆ‡æ¢è‡³: {'æ•…éšœæ¨¡å¼' if st.session_state.is_fault_active else 'æ­£å¸¸æ¨¡å¼'}")
            
        st.button(
            "ğŸ”´ æ¨¡æ‹Ÿæ•…éšœç”µå¼§å‘ç”Ÿ / ğŸŸ¢ æ¢å¤æ­£å¸¸è¿è¡Œ",
            on_click=toggle_fault
        )

        # ç”Ÿæˆå®æ—¶æ•°æ®
        t_series, current_data = simulate_current_data(
            t=2000,
            is_fault=st.session_state.is_fault_active,
            prediction_mode=(time.time() - st.session_state.t_start < 20 and not st.session_state.is_fault_active)
        )
        status_text, confidence = dl_model_inference(current_data)

        # æ˜¾ç¤ºçŠ¶æ€
        color = "green"
        if "ä¸€çº§é¢„è­¦" in status_text:
            color = "orange"
        elif "äºŒçº§é¢„è­¦" in status_text:
            color = "red"
        st.markdown(
            f"**æ¨¡å‹æ£€æµ‹çŠ¶æ€:** <span style='color:{color}; font-size: 20px;'>{status_text}</span> | **ç½®ä¿¡åº¦:** {confidence:.1f}%",
            unsafe_allow_html=True
        )

        # ç»˜åˆ¶æ³¢å½¢å›¾
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(t_series, current_data, label='ç”µæµæ³¢å½¢ (A)', color=color)
        ax.set_title("å®æ—¶ç”µæµæ³¢å½¢ç›‘æµ‹ (èˆ¹ç«¯)")
        ax.set_xlabel("æ—¶é—´ (ms)")
        ax.set_ylabel("ç”µæµ (A)")
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.set_ylim(-15, 15)
        st.pyplot(fig)
        plt.close(fig)
        
        # ... (col1: Dashboard éƒ¨åˆ†) ...

# åˆ›å»ºä¸€ä¸ªç”¨äºå®æ—¶æ›´æ–°å›¾è¡¨çš„å ä½ç¬¦
chart_placeholder = st.empty()

while True: # å¾ªç¯æ¨¡æ‹Ÿå®æ—¶æ›´æ–°
    # ç”Ÿæˆå®æ—¶æ•°æ®
    t_series, current_data = simulate_current_data(
        t=2000,
        is_fault=st.session_state.is_fault_active,
        prediction_mode=(time.time() - st.session_state.t_start < 20 and not st.session_state.is_fault_active)
    )
    status_text, confidence = dl_model_inference(current_data)

    # çŠ¶æ€æ˜¾ç¤ºé€»è¾‘ï¼ˆç•¥å¾®å¤æ‚ï¼Œéœ€è¦ä½¿ç”¨å ä½ç¬¦ï¼‰
    # å»ºè®®å°†çŠ¶æ€æ˜¾ç¤ºä¹Ÿæ”¾å…¥å ä½ç¬¦ï¼Œè¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œçœç•¥

    with chart_placeholder.container():
        # ç»˜åˆ¶æ³¢å½¢å›¾ (ä»£ç ä¿æŒä¸å˜)
        fig, ax = plt.subplots(figsize=(10, 4))
        # ... (ç»˜å›¾ä»£ç ) ...
        st.pyplot(fig)
        plt.close(fig)

        # ç»˜åˆ¶çŠ¶æ€ï¼ˆéœ€è¦é‡æ–°ç»˜åˆ¶çŠ¶æ€ä¿¡æ¯ï¼‰
        st.markdown(
            f"**æ¨¡å‹æ£€æµ‹çŠ¶æ€:** <span style='color:{color}; font-size: 20px;'>{status_text}</span> | **ç½®ä¿¡åº¦:** {confidence:.1f}%",
            unsafe_allow_html=True
        )

    # æ§åˆ¶æ›´æ–°é¢‘ç‡
    time.sleep(0.5) 

    # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½è°ƒç”¨ st.rerun()
    # ä½†è¿™ä¼šå¯¼è‡´ä¸€ä¸ªæ–°é—®é¢˜ï¼šå½“ç”¨æˆ·åœ¨ col2 è¾“å…¥æ—¶ï¼Œcol1 çš„ while True ä¼šé˜»å¡
    # å› æ­¤ï¼Œ**æ–¹æ¡ˆä¸€æ‰æ˜¯ Streamlit æ¼”ç¤ºçš„æœ€ä½³å®è·µã€‚**


    with col2:
        st.header("æ™ºèƒ½ä½“äº¤äº’ä¸­å¿ƒ (å²¸åŸºè¿ç»´ä¸­å¿ƒæ¨¡æ‹Ÿ)")

        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # é¢„è®¾å¯¹è¯å¼•å¯¼
        st.subheader("ğŸ’¡ é¢„è®¾æ¼”ç¤ºå¯¹è¯ (è¯·æ‰‹åŠ¨è¾“å…¥)")
        st.info("1. **å‰ç»é¢„è­¦**: è¯·é—®å½“å‰æ³¢å½¢èµ°åŠ¿æ˜¯å¦æ­£å¸¸ï¼Ÿæœ‰æ— æ½œåœ¨çš„ç”µå¼§é£é™©ï¼Ÿ")
        st.info("2. **è¯Šæ–­æŸ¥è¯¢**: è¯·é—®å¦‚ä½•æŸ¥è¯¢æ•…éšœç”µå¼§å‘ç”Ÿçš„æ ¹æœ¬åŸå› ï¼Œä»¥åŠèˆ¹çº§ç¤¾çš„ç»´æŠ¤è¦æ±‚ï¼Ÿ")
        st.info("3. **ç³»ç»ŸçŠ¶æ€**: è¯·å‘ŠçŸ¥æˆ‘è¾¹ç¼˜è®¡ç®—å•å…ƒçš„è´Ÿè½½ç‡å’Œç³»ç»Ÿç¨³å®šæ€§å¦‚ä½•ï¼Ÿ")

        # èŠå¤©è¾“å…¥å¤„ç†
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # ç”Ÿæˆæ™ºèƒ½ä½“å“åº”
            with st.chat_message("assistant"):
                current_status = status_text
                
                # *** è°ƒç”¨ Gemini API è¿›è¡Œç”Ÿæˆ ***
                with st.spinner("æ™ºèƒ½ä½“æ­£åœ¨è¿›è¡Œè¯Šæ–­å’Œæ¨ç†..."):
                     response = gemini_agent_response(prompt, current_status)
                
                # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                full_response = ""
                message_placeholder = st.empty()
                for chunk in response.split():
                    full_response += chunk + " "
                    time.sleep(0.01)
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
                
            # ä¿å­˜æ™ºèƒ½ä½“å“åº”
            st.session_state.messages.append({"role": "assistant", "content": response})
            # å¼ºåˆ¶åˆ·æ–°ä»¥æ˜¾ç¤ºæœ€æ–°çš„èŠå¤©è®°å½•å’ŒæŒ‰é’®çŠ¶æ€
            st.rerun() 


if __name__ == "__main__":
    main()
