import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import time
from datetime import datetime
from google import genai
from google.genai import types
from pydantic import BaseModel, Field
import json

# --- 0. ç¯å¢ƒå’Œå·¥å…·å®šä¹‰ ---

class ReportInput(BaseModel):
    """ç”¨äºç”Ÿæˆè¯¦ç»†æ•…éšœè¯Šæ–­æŠ¥å‘Šçš„å·¥å…·"""
    fault_id: str = Field(description="å½“å‰æ•…éšœäº‹ä»¶çš„å”¯ä¸€æ ‡è¯†ID")
    severity: str = Field(description="æ•…éšœçš„ä¸¥é‡ç¨‹åº¦")
    fault_type: str = Field(description="æ•…éšœç±»å‹")

class StabilityInput(BaseModel):
    """ç”¨äºæŸ¥è¯¢ç³»ç»ŸçŠ¶æ€"""

class MaintenanceInput(BaseModel):
    """æ ¹æ®æ•…éšœç±»å‹ç”Ÿæˆç»´æŠ¤å·¥å•"""
    circuit_id: str = Field(description="å›è·¯ç¼–å·")
    fault_severity: str = Field(description="æ•…éšœä¸¥é‡ç¨‹åº¦")
    maintenance_type: str = Field(description="ç»´æŠ¤ç±»å‹")

def generate_diagnostic_report(fault_id: str, severity: str, fault_type: str) -> str:
    """ç”Ÿæˆæ ¼å¼åŒ–çš„æ•…éšœè¯Šæ–­æŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    report_data = {
        "report_id": f"RPT-{fault_id}",
        "timestamp": timestamp,
        "fault_severity": severity,
        "fault_type": fault_type,
        "dl_confidence": "97.5%",
        "root_cause": "é«˜æŒ¯åŠ¨åŒºåŸŸç”µç¼†å›ºå®šä»¶è€åŒ–æ¾åŠ¨",
        "maintenance_advice": "ç«‹å³è¿›è¡Œé¢„é˜²æ€§æ£€æŸ¥ï¼Œç´§å›ºè¿æ¥ä»¶",
        "risk_level": "é«˜" if "äºŒçº§" in severity else "ä¸­"
    }
    return json.dumps(report_data, ensure_ascii=False, indent=2)

def check_system_stability() -> str:
    """æŸ¥è¯¢ç³»ç»Ÿç¨³å®šæ€§çŠ¶æ€"""
    stability_data = {
        "edge_compute_load": "38%",
        "inference_latency": "15ms", 
        "communication_latency": "45ms",
        "model_accuracy": "97.5%",
        "system_status": "ç¨³å®š",
        "last_maintenance": "2025-01-15",
        "next_scheduled": "2025-04-15"
    }
    return json.dumps(stability_data, ensure_ascii=False)

def generate_maintenance_order(circuit_id: str, fault_severity: str, maintenance_type: str) -> str:
    """ç”Ÿæˆç»´æŠ¤å·¥å•"""
    order_data = {
        "order_id": f"MO-{datetime.now().strftime('%Y%m%d%H%M')}",
        "circuit": circuit_id,
        "maintenance_type": maintenance_type,
        "priority": "ç´§æ€¥" if "äºŒçº§" in fault_severity else "é«˜",
        "required_tools": "çº¢å¤–çƒ­åƒä»ª,åŠ›çŸ©æ‰³æ‰‹,ç»ç¼˜æµ‹è¯•ä»ª",
        "estimated_duration": "2å°æ—¶",
        "safety_requirements": "æ–­ç”µæ“ä½œï¼Œç©¿æˆ´PPE",
        "assigned_technician": "å¾…åˆ†é…"
    }
    return json.dumps(order_data, ensure_ascii=False)

AVAILABLE_TOOLS = {
    "generate_diagnostic_report": generate_diagnostic_report,
    "check_system_stability": check_system_stability,
    "generate_maintenance_order": generate_maintenance_order,
}

# --- 1. æ•°æ®æ¨¡æ‹Ÿ ---
def simulate_current_data(t, fault_scenario="normal", prediction_mode=False):
    base_frequency = 50
    time_series = np.linspace(0, 2 / base_frequency, t)
    current = 10 * np.sin(2 * np.pi * base_frequency * time_series)
    current += np.random.normal(0, 0.05, t)
    
    if fault_scenario == "early_arc":
        mask = (time_series % 0.1 < 0.02)
        high_freq = np.sin(2 * np.pi * 5000 * time_series) * 0.3
        current += high_freq * mask
        
    elif fault_scenario == "severe_arc":
        high_freq = np.sin(2 * np.pi * 3000 * time_series) * 0.8
        current += high_freq + 2 * np.random.rand(t)
        
    elif fault_scenario == "motor_start":
        startup_effect = 3 * np.exp(-time_series * 2) * np.sin(2 * np.pi * 100 * time_series)
        current += startup_effect

    if prediction_mode:
        trend = 0.5 * np.exp(-time_series * 3) * np.sin(2 * np.pi * 150 * time_series)
        current += trend

    return time_series * 1000, current

# --- 2. æ¨¡å‹æ¨ç†æ¨¡æ‹Ÿ ---
def dl_model_inference(data, fault_scenario):
    max_current = np.max(np.abs(data))
    high_freq_energy = np.std(data - np.mean(data))
    
    if fault_scenario == "severe_arc":
        return "äºŒçº§é¢„è­¦ (æ•…éšœç¡®è®¤)", 97.5, "severe_arc"
    elif fault_scenario == "early_arc":
        if high_freq_energy > 0.4:
            return "ä¸€çº§é¢„è­¦ (é¢„æµ‹é£é™©)", 85.0, "early_arc"
        else:
            return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 5.0, "normal"
    elif fault_scenario == "motor_start":
        return "å¹²æ‰°ä¿¡å· (ç”µæœºå¯åŠ¨)", 10.0, "motor_start"
    else:
        return "è¿è¡Œæ­£å¸¸ (å®‰å…¨)", 2.0, "normal"

# --- 3. æ™ºèƒ½ä½“æ ¸å¿ƒé€»è¾‘ ---
@st.cache_resource
def get_gemini_client():
    try:
        GEMINI_API_KEY = st.secrets["gemini_api_key"]
        return genai.Client(api_key=GEMINI_API_KEY)
    except KeyError:
        st.error("åˆå§‹åŒ–å¤±è´¥ï¼šæ— æ³•æ‰¾åˆ° Gemini API å¯†é’¥ã€‚")
        st.stop()
    except Exception as e:
        st.error(f"åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯å¤±è´¥: {e}")
        st.stop()

def gemini_agent_response(user_query: str, system_status: dict, conversation_history: list):
    """å®Œå…¨åŸºäºGeminiç”Ÿæˆè‡ªç„¶å¯¹è¯çš„æ™ºèƒ½ä½“"""
    client = get_gemini_client()
    
    # æ„å»ºç³»ç»ŸæŒ‡ä»¤
    system_instruction = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½ä¸“å®¶ï¼Œåå­—æ˜¯"æµ·å®‰"ã€‚è¯·ä»¥ä¸“ä¸šã€è‡ªç„¶ä¸”å‹å¥½çš„æ–¹å¼ä¸ç”¨æˆ·å¯¹è¯ã€‚

å½“å‰ç³»ç»ŸçŠ¶æ€ï¼š
- ç›‘æµ‹å›è·¯ï¼š{system_status['circuit_id']}
- æ£€æµ‹çŠ¶æ€ï¼š{system_status['detection_status']}
- ç½®ä¿¡åº¦ï¼š{system_status['confidence']}%
- æ•…éšœç±»å‹ï¼š{system_status['fault_type']}
- æ›´æ–°æ—¶é—´ï¼š{system_status['timestamp']}

ä½ çš„èƒ½åŠ›åŒ…æ‹¬ï¼š
1. å®æ—¶ç›‘æµ‹ä¸æ•…éšœè¯Šæ–­
2. è¶‹åŠ¿é¢„æµ‹ä¸é¢„è­¦
3. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
4. æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§
5. åˆ›å»ºç»´æŠ¤å·¥å•

å¯¹è¯é£æ ¼è¦æ±‚ï¼š
- ä¸“ä¸šä½†ä¸è¿‡äºæŠ€æœ¯åŒ–
- å‹å¥½ä¸”ä¹äºåŠ©äºº
- æ ¹æ®çŠ¶æ€è°ƒæ•´è¯­æ°”ï¼ˆæ­£å¸¸æ—¶è½»æ¾ï¼Œé¢„è­¦æ—¶å…³åˆ‡ï¼Œæ•…éšœæ—¶ç´§æ€¥ï¼‰
- è‡ªç„¶å¼•ç”¨ç›¸å…³çŸ¥è¯†ï¼Œä¸è¦ç”Ÿç¡¬åˆ—å‡ºæ¡æ¬¾
- ä½¿ç”¨å·¥å…·æ—¶è¦è§£é‡Šå…¶ç›®çš„å’Œç»“æœæ„ä¹‰

è¯·åŸºäºå½“å‰ç³»ç»ŸçŠ¶æ€å’Œç”¨æˆ·é—®é¢˜ï¼Œæä¾›ä¸“ä¸šã€æœ‰ç”¨çš„å›ç­”ã€‚
"""
    
    try:
        # æ„å»ºå¯¹è¯å†å²
        history_text = ""
        for msg in conversation_history[-6:]:  # åªä¿ç•™æœ€è¿‘6è½®å¯¹è¯
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            history_text += f"{role}: {msg['content']}\n"
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        full_prompt = f"{system_instruction}\n\nå¯¹è¯å†å²ï¼š\n{history_text}\nç”¨æˆ·: {user_query}\nåŠ©æ‰‹:"
        
        config = types.GenerateContentConfig(
            tools=list(AVAILABLE_TOOLS.values()),
            tool_config=types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(
                    mode=types.FunctionCallingMode.ANY
                )
            )
        )
        
        # ç”Ÿæˆå“åº”
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp',
            contents=full_prompt,
            config=config,
        )
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'content') and candidate.content:
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        if hasattr(part, 'function_call'):
                            function_call = part.function_call
                            tool_name = function_call.name
                            tool_args = dict(function_call.args)
                            
                            if tool_name in AVAILABLE_TOOLS:
                                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                                tool_result = AVAILABLE_TOOLS[tool_name](**tool_args)
                                
                                # åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
                                tool_prompt = f"""
ç³»ç»ŸçŠ¶æ€ï¼š{system_status}
ç”¨æˆ·é—®é¢˜ï¼š{user_query}
å·¥å…·æ‰§è¡Œç»“æœï¼š{tool_result}

è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œç”¨è‡ªç„¶ä¸“ä¸šçš„è¯­è¨€å›å¤ç”¨æˆ·ï¼Œè§£é‡Šå·¥å…·ç»“æœçš„æ„ä¹‰å¹¶ç»™å‡ºå»ºè®®ã€‚
"""
                                tool_response = client.models.generate_content(
                                    model='gemini-2.0-flash-exp',
                                    contents=tool_prompt,
                                )
                                return tool_response.text
                
                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ–‡æœ¬å“åº”
                if hasattr(candidate.content, 'parts'):
                    text_parts = []
                    for part in candidate.content.parts:
                        if hasattr(part, 'text') and part.text:
                            text_parts.append(part.text)
                    if text_parts:
                        return "".join(text_parts)
        
        # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½ä¸è¡Œï¼Œä½¿ç”¨ç®€å•æ–¹æ³•
        return str(response.text) if hasattr(response, 'text') else "æˆ‘æš‚æ—¶æ— æ³•å¤„ç†è¿™ä¸ªè¯·æ±‚ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        
    except Exception as e:
        return f"æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€‚é”™è¯¯ä¿¡æ¯: {str(e)}"

# --- 4. ä¸»ç•Œé¢ ---
def main():
    st.set_page_config(layout="wide", page_title="èˆ¹èˆ¶æ•…éšœç”µå¼§æ™ºèƒ½ç›‘æµ‹ä¸é¢„è­¦å¹³å°")
    st.title("ğŸš¢ èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½ç›‘æµ‹å¹³å°")
    st.markdown("**æ™ºèƒ½è¯Šæ–­ Â· ä¸»åŠ¨é¢„è­¦ Â· ä¸“å®¶æŒ‡å¯¼**")
    
    # åˆå§‹åŒ–çŠ¶æ€
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant", 
                "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯æµ·å®‰ï¼Œæ‚¨çš„èˆ¹èˆ¶ç”µæ°”å®‰å…¨æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®æ‚¨ç›‘æµ‹ç”µåŠ›ç³»ç»ŸçŠ¶æ€ã€è¯Šæ–­æ•…éšœé£é™©å¹¶æä¾›ç»´æŠ¤å»ºè®®ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨å…³å¿ƒä»€ä¹ˆé—®é¢˜ï¼Ÿ"
            }
        ]
    if 'fault_scenario' not in st.session_state:
        st.session_state.fault_scenario = "normal"
    if 'circuit_id' not in st.session_state:
        st.session_state.circuit_id = "03å·èˆ±å›è·¯"

    # æ£€æŸ¥å¯†é’¥
    get_gemini_client()

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
        st.session_state.circuit_id = st.selectbox(
            "ç›‘æµ‹å›è·¯",
            ["03å·èˆ±å›è·¯", "æœºèˆ±ä¸»é…ç”µæ¿", "è´§èˆ±æ³µå›è·¯", "å¯¼èˆªè®¾å¤‡ä¾›ç”µ", "ç”Ÿæ´»åŒºä¾›ç”µ"]
        )
        
        st.subheader("ğŸ”§ è¿è¡Œæ¨¡å¼")
        scenario = st.radio(
            "é€‰æ‹©åœºæ™¯:",
            ["æ­£å¸¸è¿è¡Œ", "æ—©æœŸç”µå¼§é¢„è­¦", "ä¸¥é‡ç”µå¼§æ•…éšœ", "ç”µæœºå¯åŠ¨å¹²æ‰°"],
            index=0
        )
        
        scenario_map = {
            "æ­£å¸¸è¿è¡Œ": "normal",
            "æ—©æœŸç”µå¼§é¢„è­¦": "early_arc", 
            "ä¸¥é‡ç”µå¼§æ•…éšœ": "severe_arc",
            "ç”µæœºå¯åŠ¨å¹²æ‰°": "motor_start"
        }
        st.session_state.fault_scenario = scenario_map[scenario]
        
        st.subheader("ğŸ’¡ å¯¹è¯æç¤º")
        st.info("""
        æ‚¨å¯ä»¥é—®æˆ‘ï¼š
        - å½“å‰ç³»ç»ŸçŠ¶æ€å¦‚ä½•ï¼Ÿ
        - æœ‰æ²¡æœ‰æ½œåœ¨é£é™©ï¼Ÿ
        - è¿™ä¸ªæ•…éšœè¯¥æ€ä¹ˆå¤„ç†ï¼Ÿ
        - ç”Ÿæˆç»´æŠ¤å·¥å•
        - æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§
        """)

    col1, col2 = st.columns([3, 2])

    with col1:
        st.header("ğŸ“Š å®æ—¶ç›‘æµ‹é¢æ¿")
        
        # ç”Ÿæˆå®æ—¶æ•°æ®
        t_series, current_data = simulate_current_data(
            t=4000, 
            fault_scenario=st.session_state.fault_scenario,
            prediction_mode=(st.session_state.fault_scenario == "early_arc")
        )
        
        # æ¨¡å‹æ¨ç†
        status_text, confidence, fault_type = dl_model_inference(
            current_data, st.session_state.fault_scenario
        )
        
        # ç³»ç»ŸçŠ¶æ€
        system_status = {
            "detection_status": status_text,
            "confidence": confidence,
            "fault_type": fault_type,
            "circuit_id": st.session_state.circuit_id,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        }
        
        # çŠ¶æ€æ˜¾ç¤º
        status_color = {
            "è¿è¡Œæ­£å¸¸": "ğŸŸ¢",
            "å¹²æ‰°ä¿¡å·": "ğŸ”µ", 
            "ä¸€çº§é¢„è­¦": "ğŸŸ ",
            "äºŒçº§é¢„è­¦": "ğŸ”´"
        }
        
        emoji = "ğŸŸ¢"
        for key, value in status_color.items():
            if key in status_text:
                emoji = value
                break
                
        st.markdown(f"### {emoji} {status_text}")
        
        col1a, col1b, col1c = st.columns(3)
        with col1a:
            st.metric("ç½®ä¿¡åº¦", f"{confidence:.1f}%")
        with col1b:
            st.metric("ç›‘æµ‹å›è·¯", st.session_state.circuit_id)
        with col1c:
            st.metric("æ›´æ–°æ—¶é—´", system_status['timestamp'])
        
        # æ³¢å½¢å›¾
        fig, ax = plt.subplots(figsize=(10, 4))
        color = 'green' if 'æ­£å¸¸' in status_text else 'red' if 'äºŒçº§' in status_text else 'orange'
        ax.plot(t_series, current_data, label='ç”µæµæ³¢å½¢', color=color, linewidth=1)
        ax.set_title(f"å®æ—¶ç”µæµç›‘æµ‹ - {st.session_state.circuit_id}")
        ax.set_xlabel("æ—¶é—´ (ms)")
        ax.set_ylabel("ç”µæµ (A)")
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.set_ylim(-20, 20)
        ax.legend()
        st.pyplot(fig)
        plt.close(fig)

    with col2:
        st.header("ğŸ’¬ æ™ºèƒ½å¯¹è¯åŠ©æ‰‹")
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # å¿«æ·æé—®æŒ‰é’®
        st.subheader("ğŸš€ å¿«æ·æé—®")
        quick_questions = [
            "å½“å‰ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å¦‚ä½•ï¼Ÿ",
            "æ£€æµ‹åˆ°çš„æ˜¯ä»€ä¹ˆç±»å‹çš„æ•…éšœï¼Ÿ", 
            "è¯·ç”Ÿæˆè¯¦ç»†çš„è¯Šæ–­æŠ¥å‘Š",
            "ç³»ç»Ÿç¨³å®šæ€§æ€ä¹ˆæ ·ï¼Ÿ",
            "æ ¹æ®å½“å‰æƒ…å†µåˆ›å»ºç»´æŠ¤å·¥å•",
            "è¿™ä¸ªé£é™©éœ€è¦ç«‹å³å¤„ç†å—ï¼Ÿ"
        ]
        
        cols = st.columns(2)
        for i, question in enumerate(quick_questions):
            with cols[i % 2]:
                if st.button(question, key=f"quick_{i}", use_container_width=True):
                    # å¤„ç†å¿«æ·æé—®
                    st.session_state.messages.append({"role": "user", "content": question})
                    with st.chat_message("user"):
                        st.markdown(question)

                    with st.chat_message("assistant"):
                        with st.spinner("æµ·å®‰æ­£åœ¨æ€è€ƒ..."):
                            response = gemini_agent_response(
                                question, 
                                system_status, 
                                st.session_state.messages
                            )
                        
                        # æµç•…è¾“å‡ºæ•ˆæœ
                        message_placeholder = st.empty()
                        full_response = ""
                        for char in response:
                            full_response += char
                            message_placeholder.markdown(full_response + "â–Œ")
                            time.sleep(0.01)
                        message_placeholder.markdown(full_response)
                    
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    st.rerun()
        
        # èŠå¤©è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("æµ·å®‰æ­£åœ¨åˆ†æ..."):
                    response = gemini_agent_response(
                        prompt, 
                        system_status, 
                        st.session_state.messages
                    )
                
                message_placeholder = st.empty()
                full_response = ""
                for char in response:
                    full_response += char
                    message_placeholder.markdown(full_response + "â–Œ")
                    time.sleep(0.005)
                message_placeholder.markdown(full_response)
                
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.rerun()

if __name__ == "__main__":
    main()
